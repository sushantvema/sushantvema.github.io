---
title: "Friday July 12, 2024"
description: "Influential Work Log. Today I: "
publish: false
date_created: "2024-07-11T07:30:10"
date: "2024-07-11T07:30:17"
tags: 
  - "agenda"
---

Note: I seriously can't have caffeine after maybe 1 or 2 PM. Went to bed around 12:30 and woke up at 7:30, but I'm tired. 

# Dev Standup
- Presenting a summarization demo today at 11
- Meeting with the data team to continue our discussion of architecture

# Oof
Had to cancel the demo, had a conversion with Cyrus about deliverables. We are switching focus to an abstractive summary. The strategy is to work in meaningful stuff during differentials in times. 

I need to meet with Cyrus later today. 

# Path to Production Continued - Sagar
- There's no clear way to flag active users. Maybe by changes in follower count. Or amount of new posts. We have a cippus history, milestones, all being tracked. 
- Phase 1: Update summarizations once in 30 days. Maybe we delete and re-build the entire vector db
- All the campaigns are platform specific, 2 accounts are two different peoples. 
- Search regression suite:
  - lexical
  - keyword
  - summarization
  - filters
  - post classification
- Internal demo on Wednesdays. 
- Alex will be on vacation for two weeks after next Friday

# Today's goals
## Goals Todo
-  Use [gh/jxnl/instructor](https://github.com/jxnl/instructor) to validate JSON outputs using Pydantic
- How to evaluate JSON extractive summaries?
- Write about LLM as a judge
- Design and write the evaluation script ASAP

## Goals Completed
- create [[test_biography_data|AI Summarization Biography Test Data]]


## Yesterday evening's virtual scrum summary
Sushant:
- Many meetings
- more work on summarization + evaluation pipelines

## Yesterday's goals overflow
 

- GPU is taking 10 minutes to run. Makes more sense to run with many workers. First filtering to a group of inputs that are very small. 
- Decided to run OpenAI for embeddings. 
- AWS Bedrock foundation model setup with weaviate backend. 
- Spark is distributed computing. Need to use Pyspark 
- pandas_udf for pandas function. 
- Need to set up the endpoint again. Do it with autoscale set to zero. 
- Can use registered models as a MLflow function
- Need to hear it from the moose's ass about scale to zero. 
