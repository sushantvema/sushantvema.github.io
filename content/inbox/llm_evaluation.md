---
title: "On the evaluation of LLM-driven applications"
tags:
  - technical
---

> **It all starts with evaluation**

Automated evaluation is crucial because it measures the impact of our solution. 

**Without evaluation, all generative AI is just taking shots in the dark and hoping for the best.** We need to start with a gold standard set of input/output pairs. This MUST be generated by a human. It is a terrible mistake to use generative AI to create a gold standrd set. 

> As a best practice, you should have at least 100 pairs in your gold standard set, and thoroughly cover all categories.

The evaluation function works by using an LLM as a judge and asking the LLM to compare "correct" answers from our test set and "generated" answers from our system. 

There are a variety of prompt engineering ideas to instruct the Judge LLM. 

```python
test_prompt_template_system = """You are a detail oriented teacher.  
You are grading an exam, looking at a correct answer and a student 
submitted answer.  Your goal is to score the student answer based 
on how close it is to the correct answer.This is a pass/fail test.  
If the two answers are basically the same, the score should be 100.
Minor things like punctuation, capitilization, or spelling should 
not impact the score.

If the two answers are different, then the score should be 0.
Please you your score in a 'score' XML tag, and any reasoning 
in a 'reason' XML tag.
"""
test_prompt_template = """
Please score this submission.
Here is the correct answer:
<correct_answer>{{GOLD_OUTPUT}}</correct_answer>
Here is the student's answer:
<student_answer>{{OUTPUT}}</student_answer>"""

```

This prompt is more often used to generate a numeric score from 0-100. 

## Methods

ROUGE - Recall Oriented Understudy for Gisting Evaluation
