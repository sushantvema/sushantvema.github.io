---
title: "On the evaluation of LLM-driven applications"
date_created: 2024-07-09
date: "2024-07-16T09:14:52"
tags:
  - "technical"
  - "Large Language Models (LLMs)"
---

> **It all starts with evaluation**

Automated evaluation is crucial because it measures the impact of the solution. 

**Without evaluation, all generative AI is just taking shots in the dark and hoping for the best.** We need to start with a gold standard set of input/output pairs. This MUST be generated by a human. It is a terrible mistake to use generative AI to create a gold standrd set. 

# Learnings about open-source LLM evaluation frameworks
- [[deepeval|DeepEval]]

> As a best practice, you should have at least 100 pairs in your gold standard set, and thoroughly cover all categories.

## Methods
- [[rouge|ROUGE: Recall Oriented Understudy for Gisting Evaluation]]
