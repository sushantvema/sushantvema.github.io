---
title: "Wednesday July 10, 2024"
description: "Influential Work Log. Today I had a lot of meetings with the data team, discussed high level architecture, improved the abstractive summarization, and started to think about metrics."
publish: false
date_created: "2024-07-10T09:18:06"
date: "2024-07-11T07:29:21"
tags: 
  - "agenda"
---

# Dev Standup
- Finalized what the summary extraction schema is going to look like
- Writing a tool to validate the JSON LLM output using a python package called instruct which itself is a lightweight wrapper on Pydantic designed for checking LLM model outputs
- Data Sync meeting today
- Meeting later with the data team to discuss secrets management 
- 
- Finishing my code for calculating evaluation metrics

Cyrus:
  - Something about parallelizing pulling posts. Set up Weaviate instance. 
  - Touch base with Sushant to see progress on evaluations.

Mack:
  - High pressure demos next Monday and next week with investors and CEO Ryan. Need Devops support. Mostly presenting some slides. 7 AM PST on Monday. Need Cody, Taylor, and Adam all on board. Will have to release between T and Th next week in order to do fullstory onboarding. 

# Path to Production - Sagar
- What challenges we're making and how they will impact our milestones.
- Make sure that no one is shaken in their confidence.
- MariaDB integration is setup, networking is set up to pull from Redshift, but Redshift is going to be deprecated in our future plan anyway
- Qualities of the raw layer:
  - Replay
  - Selectively replay
  - Vacuum
  - Add necessary compression
- Qualities of the Data Lakehouse and ML Flows (Databricks):
  - There will be a lot of "noise"
  - ML Flows / Experiments
  - Delta Tables / Storage
  - Data Processing / Compute
- For long-term operational stability and maintainability, we need to have some design principles.
  - Everything in the data processing / compute section should follow a process. Let's say that Cyrus or I need a sample. That data pipeline should follow the same principles, monitoring, selective replay, etc, should follow the same qualities of any data pipeline that we need. This sample process might not need a schedule, or maybe it needs to run daily. 
  - Most of the organization lifecycle of these data pipelines should fall into this conceptual block
- We can choose to have data fetched from S3 bucket or Delta Table. Alex will make sure that his data pipelines also populate the Delta Tables.
- What we are discussing right now is only within the scope of search. If we had to extend the scope of this Delta storage and apply it generically outside to all use cases, we have to keep data probably for all time range.
- Codewise, having a subset delta table vs full delta table is not very different in implementation.
- When we do replay, we can do replay for just one day or all the way to the back. Parametrized start dates.
- For the AI summarization use case, we discussed that in the future we have to re-run the summarizations every once in a while. Dynamically select subgroups of people to re-run summarization on.
  - Can take snapshots of the data in Databricks
  - Writes are very cost effective in Databricks
- Current challenges:
  - Vector database
  - Have to point our data strategy/approach to Anj especially with respect to Weaviate POC because Anj is balancing many tasks at the same time. He probably thinks that we can make progress equally on both trains of Databricks and Weaviate. From a scope perspective, Ryan and Alex have a very clear vision on what they need to do. On the other and, Sagar, Cyrus, and I can get frequently sidetracked so we have to match progress between both angles
  - Getting distracted with the POC is a necessary evil. Because we have to figure out what our production clusters will look like. Get vector DB setup using Kubernetes so that Cyrus and Sushant are unblocked and can make quick progress. 
  - Need to be firm with the Weaviate guys, seems to be some back and forth going. 

# Data Team Weekly Sync
- Rose: Go into the project tracker and see what needs to be updated. 
- 2324: Pause this ticket, because it will delay other tickets. Having multiple workspaces will cause confusion, but in the future we will probably have a dev workspace and a prod workspace
- Setting up a self-hosted runner. 
- Create Data Pipeline Template. This is the main work and will be useful for other tickets
- The template will be code complete. Setting up different pipelines within databricks is just filling out a form. 
- Pushing on the Weaviate POC
- Two stopgaps. Internal sandbox setup. 
- Hopefully by end of day, we have our own vector database setup in Kubernetes. 
- Intelligently sampling and embeddings needs to be massively parallelized. Hopefully get it complete at the end of week. 
- The intelligently sampling process will take more time. Classification of posts is a lot more straightforward

- At what point can we start having a more frontend discussion on this

- Ryan is very stuck on the hours dashboard. Thinks that Quicksight won't be able to replicate the data. 

# AWS Secrets Management Walkthrough in Databricks

# Today's goals
## Goals

## Yesterday evening's virtual scrum summary
Sushant:
  - Established extraction schema based on business requirements, got verification from Cyrus, and working on prompt engineering strategies to get relevant chunks of input cippus.text's used for the generated summary.
  - Scheduled demo v1.5 for Thursday with Abby and and William, but it's open invite.
  - Will be presenting the new extraction schema, new abstractive summary format for narrative overview of a user, cited sources, and evaluation metrics (contextual relevance, conciseness, and a few others)
- Writing a python module to unify LLM summary evaluation metrics
Pushing evaluating on training data for tomorrow afternoon - still need to add some more profile level metadata to the sample dataset of network_account_id 's
- Writing documentation on method discovery and requirements, to be integrated with original Jira ticket tomorrow

## Yesterday's goals overflow
- create [[test_biography_data|AI Summarization Biography Test Data]]
- Use [gh/jxnl/instructor](https://github.com/jxnl/instructor) to validate JSON outputs using Pydantic
- How to evaluate JSON local summaries?
- Write about LLM as a judge
- Create local evaluation script

