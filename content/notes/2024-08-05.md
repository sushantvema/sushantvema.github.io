---
title: "Monday August 5, 2024"
date_created: "2024-08-05T05:40:42"
date: "2024-08-05T05:40:46"
author: "Sushant Vema"
tags:
  - "agenda"
publish: false  
---

# Time Tracking

## AlixPartners (3 hours)
- 5-6am: Onboarding with Databricks, Azure OpenAI studio
- 9:15-9:30, 12-12:30: Meetings - Roster Mapping Improvements, RPM Project Kickoff
- 8-8:45: Due Diligence on Roster Mapping project for Deedi

## Influential
- Told Cyrus some key points and looking ahead bits of information
- Morning meeting with databricks people
- Team sync
- one on 1 with cyrus


## Berkeley
- Sent email at 6 am about options for concurrent enrollment
- Notes from dropin advising

# Meetings

## 9:15-9:30: AP Roster Mapping Improvements, Stakeholder requests, ideas
People: Kevin Madura, Chris Baumgaertner, Mo Bhasin, Natalia Connoly, Kashyap Coimbatore Murali

Summarize what Chris and Natalia heard from 2 important stakeholders.

From Chris and David: Want capability of selecting different ML trained models. Every company has a taxonomy. Industry specific job functions and sub-functions. 

Idea: Train separate models. Drop-down of options of models. Need for a library of models trained on different subsets of the taxonomy. 

Kashyap did one- or few-shot vendor mapping model. Increased flexibility and extensibility to esoteric industries. 

They want a degree of interactivity in Deedi. Want to enter that feedback repeatedly. Chris referred to "Docu-Sign" style multiple times. 

They want to use streamlit, have to check feasibility. 

Questions:
  - Currently we use databricks as backend for endpoints and model storage.
  - All 3 modeling considerations can be satisfied by one modelling architecture for vendor mapping and roster mapping. Vendor mapping currently takes any taxonomy. We have an embedding for whatever we're categoring and then do nearest neighbors search. Dense layers on top of embeddings themselves. If taxonomy is 1000 dimensions, have to have 1 or 3 layers. Initially identity matrix. Slight modifications of the identity matrix. 
  - P2 or P3. First priority is contract parsing. Resource allocation.
  - Need some experimentation with backend. Model architecture discussion. Something running locally. 

## 9:30-10 - Product Meeting - Model Serving
People: Ahmed Bilal, Tian Tan, Mikaela Stamas, Mozgan Mazouchi, Cyrus Mohammadian, Sushant Vema

9 million posts for 17000 accounts. 3300 accounts Based on viability filter. 1.7 million posts. Further pared that down to 138 accounts. Identified 138 accounts with the most number of posts. 1/3 of all of the posts. Summaries were quick. 550000 thousand posts from the 138 accounts, classified over the weekend. 2500 dollars. 16 hours.

Only 1 error in that period of time. Error catching system. 

Bilal: Timeout of 120 seconds. Sending parallel requests to the endpoint. Had 10 nodes on the cluster. Each node was sending multiple requests, but they were in a sequence. There is a notebook, but we shouldn't worry too much about this error. This can happen in a batch endpoint when we want to push the endpoint to the max throughput. Do exponential backoff and try again. 

Have code for exponential retry used with pay-per-token. Didn't want to go with 70b model. Want to go with the small, cheaper model. Cheaper model didn't exist with a pay per token option, so couldn't prompt engineer with it. Just needs different prompt engineering. Impossible to constantly have to provision compute power and then run notebook and then shut it down every time. Keeping it on was also problematic. 

If we're working on the same endpoint there would be conflicts. Separate endpoints led to concurrency quota errors. Having to use production grade tools in dev environment.

Bilal: They want to bring provisioned throughput as pay per token. Will share scale to zero timeline. 3.1 scale to zero doesn't work because of a bug. PR from Friday will fix that. Having scale to zero allows us to more easily use production environment for development.

One or two days with prompt engineering will be cheaper. 2500 dollars for only 550k records wiht a maximum of 9 billion. Way too much. Model used right now is 70b. Average token length of a row 4-5k for verticals. 

Timing is a lot longer than benchmarking. Part of that will come down with a RAG system and other ways of reducing the cost. Don't care about the latency of the request but we want to process all rows as fast as possible. All made for batch.

For batch, we can hammer the endpoints with a lot more requests. Certain latency/throughput profile. If we sent 8 requests to endpoint, we get a latency. If we have autoscaling enabled, if we send 9 requests, we will set up 2 notes. In batch environment we can send 30 requests. Latency is lower, but overall we get more throughput. By end of this month, we will have an optimized "AI Query" client like openai batch api. Reduced cost by 2x compared to real time.

Cyrus will wrap it around with exponential backoff and mess around with parallelization. Suggestion for maximum of 24000. Then 60 requests in parallel. Can bump our concurrency level on their end. 

Ideal equilibrium state is running the scripts every day. Have to talk to engineers about what our daily load is. Will help to plan for steady state scenario. What is the throughput / second we're getting. We have the tokens for each vertical classified. Mention Mikaela on the message to ensure Bilal is informed. 

Scale to zero issue, running on weekend. Databricks will be the only platform with scale to zero for that model. 

Right now the biggest concern is costs. Product roadmap helps drive down cost as well as technical. Using POC as parallel benchmarking exercise. Still digesting that information. The usage dashboard has been very helpful to track costs. 

For timing, POC will be delivered to business by end of week. By end of this week or maximum one week. Know more in a couple days. Team is operating really lean. In-house databricks help. Nice cross-selling. 

## 10-10:30 - Berkeley Data Science Advising Meeting
Didn't get out of the waiting room. Someone named Chris emailed me back. 

## 10:30-11 - Data Science Team Sync
People: Cyrus, Alex Begg, Ryan U, Sushant Vema

Ryan is out until tomorrow. Sagar has a family emergency to attend to. Gone for the next week. 

He was running Weaviate. Not sure where his code is. He might be able to hop in early this week for a couple hour standup. All he's done is ingested into the vector store the cippus text and the network account. Mobius isn't added, demo pro isn't added, other work isn't added. Lot of work on that front. Easier to do all of this in Databricks? Already done it with cippus text. 

We're going to be working with Qdrant. Talked to co-founder, CTO. Based out of London. They claim they can definitely handle our capacity. Twitter, Cisco Systems. Since 2019. 

Cross references in Databricks are difficult to set up. 

Anj is traveling from the East Coast. Get direction from them about where we take this. We still have things to do. Expand that to 3300. Make sure the ones are good. Update prompt engineering now. Use 3.1 70b. 

Cyrus: Processed 550k accounts. Final plan is to get 900 million records into the vector store. Just not going to be viable. Leverage the influencer viability project. Only ingest "viable" accounts. In Youtube, 9 million posts and 17000 accounts and counting. 17000 -> 3300. 9m posts -> 1.7. Okay from business and everybody that this is the approach we take. 

Sagar was having trouble uploading things to weaviate. Sample accounts with the most number of posts. 550k posts which need to be ingested. 

Come up with a process. Haven't set up data change and lineage process. 

Supposed to have this done August 1-5. Now there's a new project called Sephora project. Tight deadline. Okay if search gets sidelined a little bit. 

When we're doing vetting, we need entire timelines. 

Difficulty with cursor-based ETL. Shifting to CDC (Changed Data Capture) for MariaDB side of it. 

Something about Kafka streaming set up. The complexity will always be on the MariaDB side. It doesn't matter whether we have 5 days or anything on MariaDB. MariaDB's processing should be done in a matter of minutes. Getting the latest changes is one of two ways. Cursor-based or log-based. If the only option is cursor-based and we don't have the proper options to filter, it will be a huge pain. 

When we start incorporating data from Publicis, everything needs to be moving efficiently. Will talk to Taylor. Only a couple configuration changes on production database. 

## 11-11:30 - Cyrus Sync
Today: Ensure that the 138 summaries we have are top notch. Those are the best of the best. Accounts with more than 5000 posts AND viable. 
May give a different project for this week. Other project might be updating the vertical classification: The prompt has random examples of positive cases. Same prompt for every post that gets processed. Instead of exposing each input to a litany of posts. 
Vector store reads are cheap? Can't find a skew that represents just the vector store. 
  - Cost of the compute
  - Vector store is simply storage. 
  - Two choices about inputs: Tell it to use a vectorizer. OpenAI, Llama, etc. Or BYOB. 
  - How much time and money does every unit of retrieval cost?

Work on summaries for today. At 500000. Can set up FAISS. 

Spoken with Anj about this. From the perspective of the work, great work. We would be lucky to hire me right now full time. Given the purchase with Publicis, we're not allowed to put out offers right now. Hear back about when hiring. Give a 6 month contract and negotiate and negotiate a salary for full time. If at some point we get positive news about. This way we can still continue working, and then we can negotiate a more steady state salary for me. 

Get matching offers. While you're here as a contractor

Final status of 138 summaries. Formal code review before the 3300. 

## 12-12:30 - AP RPM Project Kickoff
People: Natalia, Sushant

One thing about this space. This Databricks space shouldn't be used for development. 

Most files will contain bars for RPMs. Lower range and upper range. Sometimes they put other company's RPMs 

Setting up the database. Ask whatever other space we can use for development that doesn't involve stuff. 

OpenAI keys. For each databricks. Secrets management system. 

Talk to Kevin Madura. Cloud team: Joe Demianaoche. Absolutely.

Sharepoint. 

PTO from tomorrow to Friday. Include Kevin and Kashyap. 

Access to AP official taxonomy. Currently stored in sharepoint and. Not the main priority at the moment but

Best person is Kashyap and Mo Bhasin to learn about the Deedi project. Application developed primarily for due diligence. Companies about to go through with bakruptcy or about to start. Going through lots of documents. Streamline and summarize current situation. 

Until now, the entire DD process is manual. Deedi does that with GenAI. Create a virtual data room. 

People loved the tool, but now there are many extensions, "Deedi tools". Roster mapping is now part of Deedi. Standardized taxonomy. 

Kashyap developed vendor mapping tool. On-the-fly mapping that uses integrated web search. Contract project. 

Mo, Kashyap and Natalia write code 100% of the time and some pitches. Very much hands on. Head of AI team is Hoyan. He's in charge of structuring team and client assignments. Luca Ridalfi. Not very hands on. Greg Adams. In between. Hands on work and business development. AI group in the Performance and Technology team. At least 100 people in the PT org. 

## Chat with Kashyap
Meeting every Friday with Natalia. Natalia is the pipeline. 
Research group. Dandelion Dreams. High level groups with multiple projects. Alix Partners has a bunch of clients. Each client has separate clients. Ulysses is the main development environment. 

Chris Baumgaertner is not very visible. Look at the data and the task. 

Two mapping tasks: job titles to predetermined taxonomy. Vendor mapping. If I give 10 or 20 rows mapped already, improve the task. Natalia is using some sentence transformer model. Embedding model with classification layer on top of it. 



## Call Audio Rental Company
- 4300. 
- Two speakers might not be enough. 
- Audio for a relatively small venue. 
- 1500, 
- cparrish@musson.com
- Include full information with name, billing address, phone number, dates we need equipment for. 

# Grad Party
- [x] Call Musson audio rental company
- [] Order the cake from Raley's Sunol and decide on the text and flavor
- [] Start writing most of graduation speech
