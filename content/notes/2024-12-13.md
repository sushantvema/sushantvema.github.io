---
title: "Friday December 13th, 2024"
date_created: "2024-12-13T09:19:48"
date: "2024-12-13T09:19:51"
tags:
  - agenda
publish: false
---

# Todo
- Meeting with Kevin and Andre about citations
- Submit change of domain emphasis
- Check remaining requirements for middle east class
- 

# Checkin with Alex
- Added the checkpointing notebook. Create schema query. Drop search index schema if exists. 
- Consistency 

#  Checkin with Cyrus
- Wanted a better understanding of what's happening

# Checkin with Cyrus and Ryan
- Ryan: playground throwing errors. Briefing project. Try with the Marcel GPT. Try with normal chatgpt. 
- Ryan: Sentiment analysis, top 5% stuff. They'll meet later today, but work on that early next week, Monday. 
- Something about campaign briefs. Need to talk to them about 
- Take top 5 percent. Apply the sponsored filter. Collect the group of accounts we want to process. Of all of the remaining accounts, rank order them by post frequency and break them into n-tiles, randomly select from each quintiles. 
- Random 50 from each n-tile. influencer or brand. For each n-tile percentage of accounts that are non-influencers. Keep a record of each of those. Top 100, 75% of them were non-brands. Keep doing that until we hit the 50% mark. 
- Summaries integration: Looked at the SQL code for change data feed process. Didn't see anything in pyspark. RAG is connected to the preprocessing notebook, connected to the openai batch inference code. connected to the embedding process. Looking for the best approach to ensure that the basic incremental ETL 
  - enable the CDf
  - separate the summaries and the flat table
  - separate 

# 10:30 Alex/Cyrus/Sushant Summaries ETL Discussion
- Inference ETL workflow. Trigger from qdrant. cippus and network_account collections have to yield an update trigger. 
- rag -> preprocessing -> batch inference -> embeddings 
- Make this asynchronous
- When the batch is expired, it will tell you how many individual requests are completed. 
- Structured streaming between
- Provide some example links. 
- Idea: use s3 to create queues. Use checkpointing. Anything we need to add to the queue, add new files. Have to make sure there are no duplicates. s3 folders, workflow queuing programmatically. 

# 11am Silver ETL Work

- try semantic chunking in Deedi using langchain's semanatic chunker with openai ada endpoint
