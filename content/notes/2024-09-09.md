---
title: "Monday September 9th, 2024"
date_created: "2024-09-09T09:03:14"
date: "2024-09-09T09:03:17"
publish: false
tags:
  - "agenda"
---

# Personal
## Top of Mind
- Submit I9 documents for Influential/Publicis
- Call Kaiser Permanente Recruiter tomorrow and schedule interview for Wednesday
- [x] Set up Influential ADP thing
- [x] Pay Devang for Onam Sadhya

# Meetings
## Search 2.0 Sprint Planning Meeting
- Databricks Infra Setup. We have a dev environment, but we need environments for staging and prod. 
  - Is the environment in our VPC? As long as it's within our AWS VPC, we're good. Used the quickstart method. Maybe we're missing some enterprise security features. 
  - Our bronze layer needs to have a structure. This has been way pending. We have a layout finished in S3 for dev, but needs to be finalized. 
  - All tasks need to have at least one other person review the task. 
- Create Data Pipeline Tempalte for Change Data Capture
  - Template is completed and has been used for a lot of tables already.
  - PR is also merged. Alex will do a demo of this on Friday and then ready for review.
- Setup dev qdrant vector database. Setup by Taylor himself. Used the existing Terraform scripts by qdrant. Asked Taylor to get the Terraform scripts in Github. Will add a task and assign it to Taylor.   
- Data pipeline to classify posts (tagging). Explore fine-tuning on the GPT models. There's no reason why we shouldn't. Have bandwidth until Thursday. Cyrus has a good handle of GPT finetuning. 4o mini is free finetuning until September 4th. 
- Index mobius in vector database. Yet to be done. Haven't started this. Mobius data is only the Youtube data in July. The pipeline aren't completed for this yet. Last thing to be done this week. Anj has asked us to embed those to be semantically. For now using qdrant vectorizer. Wondering why this is part of the Infra Setup epic. 
  - Creating the index as well as indexing the data.
- Run evaluation pipeline on search. Have to make sure that search results are returning meaningful queries. 
- Define guardrails/security around self-hosted LLM. Sagar suggests leaving this out for now. We may or may not use a self-hosted LLM. We'll know by the end of the week. 
- Data pipeline for embedding summarization. Not sure whether Sagar can get to that
- Set up monitoring for data ingestion pipelines. Cluster health is something we're interested in. No out of the box solution. Is this something we can give to the new DevOps person. 
- Data validation for vertical labels. 
- Data validation for summaries can be accomplished by this week. 
- Integrate the search endpoint with UI. This has a larger story with a variety of subtasks to it. There is some work to be done with understanding the schema, we still have to talk to Cody. Pushed meeting with Cody to Wednesday. 
- Sushant will be meeting with Cody on Wednesday
- Is there a ticket on reranking / scoring? There's some score that they market which they have a contractual obligation to show. 
- Log performance metrics. Removed, for Ryan.
- Permissions to start a cluster for AI Assisted Search. 
- Today the data will be ready. Ironed out all of the vector DB issues on Friday. TODO: Add a ticket. Add this to the sprint.
- There's a sprint called Search 2.0 backlog. Network and cippus tables are now loaded in. The only thing that will be missing are demo pro and the mobius. Cippus text will be done by end of day today. 

## Data Science Standup
- Alex: Cippus and cippus text for all the networks until today loaded in. Working on daily ETL for it. Mobius and demo pro data remaining. 
  - These are in Delta Tables. Raw layer tables. 
  - Working through some solutions for the three biggest tables. Was the solution that Cyrus discussed last week what helped to finalize this project? Just used Airflow's s3 bucket. 
  - Anj: If this can't get working today for one of the tables, get some input from Databricks folks. We have architecture advisory hours which we can tap into if we're making slow progress. 
  - At some point we need to find out why he's having these issues. If I have 96 partitions, 8 of them work fine, some thing some thing then entire pipeline fails.  
  - We pulled in 40 terrabytes of data using that optimization. Using Pyspark's connect. Let's take it offline. 
- Steve: Moving back to 1 week sprint model in order to consolidate what we can achieve in one week. Getting Bruno on this call 
- Cyrus: Cyrus and I will be able to get the RAG refactoring completed by sometime tomorrow. Let that simmer until all of the data is there. Sushant will revert his attention to ETL processes. Setup a meeting with Sushant and Cyrus to discuss ETL stuff. Cyrus has a script that embeds 9 million posts in under 2 hours and send them to an S3 bucket. Spend a little bit of time finetuning the mini model using the OpenAI SDK. Reconvene end of the week to check out my template on the OpenAI batch inference. Get some classification done while working on stuff. Implementing the change data capture in our own notebooks. Start with the RAG application. 
- Taylor: Disaster recovery environment. First stages of the audit this week. 
- Sagar: Fixed all the issues with qdrant search last week. Yesterday late night had conversations with qdrant team. Given us some direction on the different search methods they have. One of the developers / solution architects has sent some code. Still asked them to schedule a call to go into detail for approaches to go into production. Results look really good. Sent an email on late Friday. All queries are working with advanced filters. Looking at Radius search. 
- Ryan: Look into Airflow errors we got over the weekend. 
- Kat: No updates. Last applicant batch has been appended. 
- Anj: Now that the data is there on the cippus. 
- Sagar: Other thing for me to pickup since we have bandwidth is to start documenting GraphQL schema for integration. 

## Cyrus, Ryan, Sushant Sync
- Ryan knows the data warehouse well. One of the things right now is that Sagar needs a list of all of the tables used for mobius. What columns are needed to get the mobius tags. 
- On the other end, we're currently working on getting OpenAI process integrated into our flow. We're going to have to start working on a changed data capture process. One of the things that would be good for Ryan â€“ is there a way to integrate workflows into other workflows. There's one workflow in databricks that does RAG classification, another workflow that does semantic search, etc. 
- We need someone to have a bird's eye view into what we're doing. Recommendation for Ryan to get into Draw.io and create a diagram. 
- At the minimum we need to explore what Databricks has. Right now, we're going to have a lot of different clusters used suboptimally. We need to be able to share clusters for these different processes. We want to make sure that the Terraform process is more of a help than a burden. 
- Identify how to check the memory size of the table I'm working with. Make sure that each batch is under the max memory size. 
- Save a file as a tmp file in memory. Make the chunks no larger than 90MB. 250k rows need to be processed every single day. Spark dataframe. 
- Cyrus is going to work on finetuning and will share his script. Can help with edgecases. 

# Work
## Influential - Set up parallel batch requests and intelligent file creation for memory limits


