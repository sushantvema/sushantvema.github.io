---
title: "Wednesday August 7, 2024"
date_created: "2024-08-07T07:44:33"
date: "2024-08-07T07:44:38"
author: "Sushant Vema"
tags: 
  - "agenda"
publish: false  
---
# Top of mind
- EGT extension form
- Call Guitar Center when they open at 11
- Else finalize payment for Musson
- Pick up equipment from Srini Uncle in the evening

# Time Tracking

## Alix Partners

## Influential

# Meetings

## Databricks Checkin Meeting

## Call Guitar Center
- Premium enhanced package. 
- Two 12"'s and 18 inch subwoofer. 2 Speaker stand. Cables for speakers. 3 XLR cables 25 foot. 
- Can buy patch cable. 
- We need a multi 
- Harbinger's specifically made for them. People want to buy them. As good as QSC's. Premium speakers most DJs use. 
- Sub is on wheels. 
- $189.99. Deposit of $189.99. 
- Sidney

## Call Chris Parrish at Musson

## Data Team Weekly Sync
- All the youtube data is ingested in the system. Identified variable and threshold on that variable to separate viabile vs non-viable accounts.
- Threshold brought us from 9 million posts to 3300 accounts. 
- Further cut down to accounts with the most posts, top 5. 24 accounts. 170000 posts for these 24 accounts. 
- Alpha is a continuous value between 0 to 1. 0 is pure keyword search, 1 is pure semantic search?
- Exact phrases are frequently used more than key words. No clear way to enforce exact match phrases in Weaviate. 
- There are some tokenization settings. Default is word tokenization. 
- The score is normalized. The spectrum on which it's normalized is unclear. The score is NOT normalized against the output. Currently set at 10 results for the demo. 
- In UI there might be a separate input for the AI and separate input for exact match search. Thumbs up by Alyssa. Alex suggests that you could mess around with the `operator` field of the Get JSON in Weaviate. 
- Anj asks: Does Weaviate return the total number of matches? Don't know how the algorithm determines what counts as a match. 
- Anj expected a full-powered semantic search with sentence inputs. We're almost there already, but Cyrus and I agree that there will need to be an AI layer between the raw query and queries that hit the Weaviate endpoint in order to remove noise.

## Meeting with Chris Data Science Advising
- EGT Graduation requires 4 year plan signed by major advisor, need to get an appointment for that
- June 1 was the deadline for readmission. How to proceed for that?
- Not sure what the timeline will be for this + enrollment.
- SHIP Waiver by August 15
- Application for RCL timeline
- May have an option to still get me in. Have to check with the registrar. The challenge is that we're in to phase 2. Close to the adjustment.
- Academic notice instead of probation.
- Recommend we do this all part of the same process. Request readmission. Graduation term to fall. Domain emphasis change. Biggest concern is timing.
- Degree requirements and policies page. Academic policies is where readmission criteria will live.
- https://registrar.berkeley.edu/wp-content/uploads/2021/03/readmission_form.pdf
- https://registrar.berkeley.edu/registration/readmission/
- The college would advocate on my behalf if there was flexibility to do so.
- Turnaround time in a couple of days. Deadline to apply for readmission for spring is November.
- Concurrent enrollment discussion
  - Meant to discuss enrolling at another institution. 

## 1-1 with Cyrus
- Finishing the 3304 accounts summarization instead
- In the databricks channel there is a script for production. There's something called an Autoloader. Allows us to stream in data from a source object. Can be table in Delta Lake and 
- Input table is network_account_table. 
- Have to figure out the cadence of re-running accounts. 
- They will get triggered when they're in workflows. 
- Follow the vertical classification process. https://dbc-35bfa1f1-1292.cloud.databricks.com/jobs/540125424489955?o=1016658646341465. 
- 10 workers. Figure out how many nodes. Minimum 8 cores. 
- Understanding how to generate workflows. Databricks without workflows is fluff. Absent stronger knowledge of Apache Airflow, knowing workflows is highly valuable. 
- Maximum concurrence runs. 
- Do not pick existing clusters. Pick job clusters. 50% cheaper. 
- Don't use job parameters
- Do job clusters get passed. Way around 
- Orchestration notebook
- Spark suffers with accumulation of garbage in the environment and on the cluster. 
- Maybe we store prompt in separate notebooks. VC with github and internal version control with databricks notebooks. Notebooks have had so much code in them that version control got deleted. 
- Test with the general purpose cluster. After 
- Eventually everything will be terraformed. 
- Converts infrastructure 

## Discussion about Cal Poly SLO 
- September 17. 
