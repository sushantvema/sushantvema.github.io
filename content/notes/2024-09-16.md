---
title: "Monday September 16th, 2024"
date_created: "2024-09-16T08:32:43"
date: "2024-09-16T08:32:46"
tags:
  - "agenda"
publish: false  
---

# Personal

# Work

## Influential - Qdrant Node SDK Meeting
- Some of the documentation was made for the other library. Sagar will be here in a couple minutes. 
- Anush handles qdrant's integrations.
- Atita. Solutions side of things at Qdrant.
- Meeting was specifically focused around the node SDK and the questions around it.
- Cluster is setup and testing. Ongoing monitoring process.
- Wrapping up some finetuning in the qdrant database.
- Yesterday night Sagar was able to get everything working. All the profiles are ingested and run all of the semantic and hybrid searches are ran using dense and sparse vectors.
- Looking at the sparse vectors, there is an old reference.
- There was a workshop repository that Casper wrote. Had to combine all the ways together in order to create the final version of the code.
- Speaking to Anj in 2 hours time. 1) Review the code with them and see if everything looks good. 2) Dive into finetuning of the data structure. 3) Laying down the foundations for productionalizing the setup for hybrid cloud. TLS, certificates, etc. Getting the query endpoints setup in Node.js for our frontend engineers.
- Maximum 40 fields in the final DB. The writes we see are not going to be heavy every day. It's very sporadic. The first initial load will be high, but that's a one time cost. The frequency of batch refreshes will be once a day. Kind of like an upsert. A few new ones as well as updates of existing ones.
- Daily 340 accounts added to the system. 240000 posts daily. Backfills and reingestions of previously updated data.
- Going to maintain binary quantization for cost reasons. Started with 128 and then 324 dimensions, and then 512. Every time the dimensions increased, the accuracy increased as well. Should decide between 512 and 768.
- Profiles summaries are universally around 250-300 words. Word on the street is that 512 should be good enough dimensions for that. We were originally at 1500, so maybe 768 is more reasonable.
- Long back we decided to park the 1536 approach. It was very evident we didn't need to.
- 768 we tested was MPNET.
- There are two sessions. One session about core data science, core search engine. Second section about productionizing.
- Query side, data model/data schema side, advanced filtering and querying, right dimension/good model for production . So far FastEmbed looks good with 512.
- Whatever model that is selected for the final application, should be based on our criteria for domain.
- It should be able to understand English. High quality understanding of grammar. No functional domain specific. 25% of posts are non-english.
- Atita recommends BGE 304 dimensions. Quantization is one of our parameters. Quantization shows its benefits with models with higher dimensions.
- We want to go with very high accuracy of a non-binary quantization. First phase not in favor with quantization since we want to show a strong accuracy product first.
- Backfill with high dimension is not a high cost exercise. With just 3000 network accounts, it's nothing.
- Production sizing that Karim did with us is with binary quantization. They got the TCO because of that.
- Karim asked Cyrus to add dimension vs quantization question to the document to send for the session. This is important

## Data Science Standup
- Alex: Over the weekend, repopulated the cippus_text. Some error came up. First time, realized that the ETL wasn't properly using the updated column. Blocked on demo_pro until there is an index. Done by first half of this week. Adam is doing the index. Waiting to confirm with Taylor. Thumbs up from DevOps before he's done that. Will get on that today. 
- Ryan: Spent some time reviewing code for the dashboard. Meeting with Adam at 11:30 to go over root causes of duplication. 
- Sagar: Weekend populated the qdrant. Today wrapped up all of the posts. Testing all of the queries. Scheduling 30 minutes today or tomorrow to demo the search. No blockers. 
- Cyrus: Iterating over the model. For end of this week, have the vertical classification module completed. Knowing the exact levers to pull to scale properly. 
- Sushant:
- Bruno: Last week completed development training. All accesses he needs. Workspace setup is complete. Taking new tasks from Taylor. 
- Taylor: Index will take 1.5 to 3 days. No panic until 4 days behind. Check with Adam when we meet. 

## Database Discussion
- Two reasons why we delete records: Accounts that shouldn't be added to our system in the first place. Mostly twitter accounts that are pornographic in nature. Second reason is legal requests. Influencer requests to remove their account. Never happened before. Another reason why records are deleted is because the placeholders are suboptimal. When the placeholder is updated with the real account, the placeholder might get deleted. There's some sort of logging for deletions. An archival idea got a bunch of blowback. Instead of hard delete, records should be stored in an archive table. 
- There was a new network_account_bio record for a network_account with a new id. The previous one that we had ingested was no longer in MariaDB. Looks like it was deleted and re-added?
- Maybe the influencer removed the bio from their network and then we scanned when the bio wasn't there, so it was deleted. 
- network_account_bio, cippus, and cippus_text
- Adam can create tables to log deletions. We can log bio deletions, final versions before deletion etc can be automatically logged. 
- Image recognition is possible with mobiusv3. We can probably keep all versions
- cippus_media is tied to an s3 bucket. Image recognition is done on the s3 bucket. 
- TLDR; Need to implement some sort of changed data capture / deletion logging / cleanup process

## Influential - Data Science Sprint Planning Meeting
- What is going to be the formal code review process?
- Focus on what the team can commit this week for Friday's demo.
- Review DS-97

## Influential - Fix Openai Batch Inference 
- Questions about the medallion system, how that would work for this. 
- 
